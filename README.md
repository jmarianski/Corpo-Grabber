# Corpo-Grabber

Required utility: wget (or httrack if you want to fiddle with code)

Utility web page for grabbing text corpora from the internet and stores them.

Currently you can download entire site with "advanced download". "Load" function allows applying filters to downloaded sites.

It works like that:

1) You download site

2) You load project

3) You're presented with visual representation of site. 

4) You're tasked with defining elements, that you'd like to extract. 

5) When you define them, you then select text elements that best describe desired element. 

6) When you select them, the exact pattern will be saved and it will be applied to other subsites

7) After all of the sites have been parsed with the pattern, the site is saved to desired format (pre-morph)

Right now, project is in research mode, meaning if you fill up the fields below "do bada≈Ñ" with unique tags that define these elements, you'll be presented with data comparing the use of selector algorythm and positioning algorythm.


#TODO by now:

1) Fix deduplication

2) Saving to CCL format


This is a project for master thesis. If you'd like to contribute, please acknowledge me with sources you've used to manipulate the code. If you'd like to request a feature, please use the apropriate option on github.

Thank you and have a nice day!
